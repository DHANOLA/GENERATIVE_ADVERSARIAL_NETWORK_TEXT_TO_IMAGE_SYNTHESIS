{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GENERATIVE TEXT TO IMAGE SYNTHESIS PART 2 UPTO EPOCH 25","metadata":{}},{"cell_type":"markdown","source":"## IMPORTING LIBRARIES","metadata":{}},{"cell_type":"code","source":"import math #FOR MATHEMATICAL FUNCTIONS\nimport tensorflow.compat.v1 as tf #USIN VERSION 1 OF TENSORFLOW USING COMPATIBILITY OF TENSORFLOW 2\ntf.disable_v2_behavior# WE ARE DISABLING THE USE OF TENSORFLOW VERSION 2 AND USING VERSION 1\ntf.disable_eager_execution()#DISABLE THE EAGER EXECUTION AS IT WAS CAUSING SOME TROUBLES DURING TRAINING\nfrom tensorflow.python.framework import ops#FOR SOME TENSORFLOW OPERATIONAL FUNCTIONS\nimport skimage#SCKIT LEARN IMAGE HANDLING LIBRARY\nimport skimage.io\nimport imageio#READING IMAGE FILES\nimport skimage.transform#TRANSFORM IMAGES PROPERTIES\n!pip install tf_slim \nimport tf_slim as slim# HELPS IN BUILDING COMPLEX MODELS EASILY\nimport pickle# FOR LOADING THE PICKLES FILES FROM PART 1\nimport scipy.misc\nimport random#FOR RANDOM VALUES\nimport os#FOR OS OPERATIONS\nimport shutil#FOR HIGH LEVEL FILE OPERATIONS\nimport numpy as np#NUMERICAL CALCULATIONS LIBBRARY\nfrom os.path import join#TO JOIN FILE PATHS\n# to show the loss functions graph over time\n!pip install -U tensorboard","metadata":{"execution":{"iopub.status.busy":"2023-03-30T22:17:31.939238Z","iopub.execute_input":"2023-03-30T22:17:31.939795Z","iopub.status.idle":"2023-03-30T22:17:54.596279Z","shell.execute_reply.started":"2023-03-30T22:17:31.939742Z","shell.execute_reply":"2023-03-30T22:17:54.594791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## UTILITY FUNCTIONS","metadata":{}},{"cell_type":"code","source":"\n# CLASS FOR BATCH NORMALIZATION \nclass batch_norm(object):\n    \n    def __init__(self, epsilon=1e-5, momentum = 0.9, name=\"batch_norm\"):\n        with tf.variable_scope(name):\n            self.epsilon = epsilon\n            self.momentum = momentum\n\n            self.ema = tf.train.ExponentialMovingAverage(decay=self.momentum)\n            self.name = name\n\n#     FUNCTION FOR TRAINING THE GAN MODEL ON THE SKIP THOUGHT VECTORS\n    def __call__(self, x, train=True):\n        shape = x.get_shape().as_list()\n\n        if train:\n            with tf.variable_scope(self.name) as scope:\n                self.beta = tf.get_variable(\"beta\", [shape[-1]],\n                                    initializer=tf.constant_initializer(0.))\n                self.gamma = tf.get_variable(\"gamma\", [shape[-1]],\n                                    initializer=tf.random_normal_initializer(1., 0.02))\n\n                try:\n                    batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n                except:\n                    batch_mean, batch_var = tf.nn.moments(x, [0, 1], name='moments')\n\n                ema_apply_op = self.ema.apply([batch_mean, batch_var])\n                self.ema_mean, self.ema_var = self.ema.average(batch_mean), self.ema.average(batch_var)\n\n                with tf.control_dependencies([ema_apply_op]):\n                    mean, var = tf.identity(batch_mean), tf.identity(batch_var)\n        else:\n            mean, var = self.ema_mean, self.ema_var\n\n        normed = tf.nn.batch_norm_with_global_normalization(\n                x, mean, var, self.beta, self.gamma, self.epsilon, scale_after_normalization=True)\n\n        return normed\n\n\n# FUNCTION TO COMPUTE BINARY CROSS ENTROPY\ndef binary_cross_entropy(preds, targets, name=None):\n    eps = 1e-12\n    with ops.op_scope([preds, targets], name, \"bce_loss\") as name:\n        preds = ops.convert_to_tensor(preds, name=\"preds\")\n        targets = ops.convert_to_tensor(targets, name=\"targets\")\n        return tf.reduce_mean(-(targets * tf.log(preds + eps) +\n                              (1. - targets) * tf.log(1. - preds + eps)))\n# Concatenate conditioning vector on feature map axis\ndef conv_cond_concat(x, y):\n    x_shapes = x.get_shape()\n    y_shapes = y.get_shape()\n    return tf.concat(3, [x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])])\n\n# FUNCTION FOR CONVOLUTION2D LAYER\ndef conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n           name=\"conv2d\"):\n    with tf.variable_scope(name):\n        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim], initializer = tf.keras.initializers.glorot_normal())\n        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n\n        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n\n        return conv\n\n# FUNCTION FOR DECONVOLUTION2D LAYER\ndef deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02,\n             name=\"deconv2d\", with_w=False):\n    with tf.variable_scope(name):\n        w = tf.get_variable('w', [k_h, k_h, output_shape[-1], input_.get_shape()[-1]], initializer = tf.keras.initializers.glorot_normal())\n        try:\n            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape,\n                                strides=[1, d_h, d_w, 1])\n\n        except AttributeError:\n            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n                                strides=[1, d_h, d_w, 1])\n\n        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n\n        if with_w:\n            return deconv, w, biases\n        else:\n            return deconv\n\n# FUNCTION FOR LEAKY RELU FUNCTION\ndef lrelu(x, leak=0.2, name=\"lrelu\"):\n    return tf.maximum(x, leak*x)\n\n# FUNCTION FOR LINEAR LAYER\ndef linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0,\n           with_w=False):\n    shape = input_.get_shape().as_list()\n\n    with tf.variable_scope(scope or \"Linear\"):\n        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n                                 tf.keras.initializers.glorot_normal())\n        bias = tf.get_variable(\"bias\", [output_size],\n            initializer=tf.constant_initializer(bias_start))\n        if with_w:\n            return tf.matmul(input_, matrix) + bias, matrix, bias\n        else:\n            return tf.matmul(input_, matrix) + bias\n\n\n\n# FUNCTION FORCALLING THE GENERATOR\ndef get_gt(batch_size, classes, real=1, name=\"gt\"):\n\n    with tf.variable_scope(name, reuse=None):\n        r_f = tf.get_variable(\"rf\", [batch_size, 1],\n                               initializer = tf.constant_initializer(\n                                   real))\n        gt = tf.concat(1, [r_f, classes], name = 'gt_concat_classes')\n        return gt","metadata":{"execution":{"iopub.status.busy":"2023-03-30T22:17:54.599585Z","iopub.execute_input":"2023-03-30T22:17:54.600639Z","iopub.status.idle":"2023-03-30T22:17:54.632248Z","shell.execute_reply.started":"2023-03-30T22:17:54.600597Z","shell.execute_reply":"2023-03-30T22:17:54.630880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FUNCTIONS FOR IMAGE PROCESSING","metadata":{}},{"cell_type":"code","source":"\n#FUCNTION TO LOAD ALL THE TRAIN IMAGE FILES\ndef load_image_array_flowers(image_file, image_size):\n    img = skimage.io.imread(image_file)\n    # GRAYSCALE THE IMAGES\n    if len(img.shape) == 2:\n        img_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n        img_new[:,:,0] = img\n        img_new[:,:,1] = img\n        img_new[:,:,2] = img\n        img = img_new\n\n    img_resized = skimage.transform.resize(img, (image_size, image_size))\n\n    # FLIP HORIZONTAL WIRH A PROBABILITY  OF 0.5\n    if random.random() > 0.5:\n        img_resized = np.fliplr(img_resized)\n\n\n    return img_resized.astype('float32')\n\n#FUCNTION TO LOAD ALL THE VALIDATION/TRAIN IMAGE FILES\ndef load_image_array(image_file, image_size,\n                     image_id, data_dir='Data/datasets/mscoco/train2014',\n                     mode='train'):\n    img = None\n    if os.path.exists(image_file):\n        #print('found' + image_file)\n        img = skimage.io.imread(image_file)\n    else:\n        print('notfound' + image_file)\n        img = skimage.io.imread('http://mscoco.org/images/%d' % (image_id))\n        img_path = os.path.join(data_dir, 'COCO_%s2014_%.12d.jpg' % ( mode,\n                                                                      image_id))\n        imageio.imwrite(img_path, img)\n\n    # GRAYSCALE\n    if len(img.shape) == 2:\n        img_new = np.ndarray( (img.shape[0], img.shape[1], 3), dtype = 'uint8')\n        img_new[:,:,0] = img\n        img_new[:,:,1] = img\n        img_new[:,:,2] = img\n        img = img_new\n\n    img_resized = skimage.transform.resize(img, (image_size, image_size))\n\n    # FLIP HORIZONTAL WIRH A PROBABILITY 0.5\n    if random.random() > 0.5:\n        img_resized = np.fliplr(img_resized)\n\n    return img_resized.astype('float32')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T22:17:54.633801Z","iopub.execute_input":"2023-03-30T22:17:54.634251Z","iopub.status.idle":"2023-03-30T22:17:54.648875Z","shell.execute_reply.started":"2023-03-30T22:17:54.634208Z","shell.execute_reply":"2023-03-30T22:17:54.646806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FUNCTION TO CREATE GAN MODEL","metadata":{}},{"cell_type":"code","source":"\n# CLASS FOR CREATING THE GAN\nclass GAN :\n\n    \n#     FUNCTION FOR GAN MODEL INITILIZATION\n    def __init__(self, options) :\n        self.options = options\n\n#     FUNCTION TO BUILD THE MODEL\n    def build_model(self) :\n\n        \n        print('Initializing placeholder')\n        img_size = self.options['image_size']\n        t_real_image = tf.placeholder('float32', [self.options['batch_size'],\n                                      img_size, img_size, 3],\n                                      name = 'real_image')\n        t_wrong_image = tf.placeholder('float32', [self.options['batch_size'],\n                                       img_size, img_size, 3],\n                                       name = 'wrong_image')\n\n        t_real_caption = tf.placeholder('float32', [self.options['batch_size'],\n                                        self.options['caption_vector_length']],\n                                        name='real_captions')\n\n        t_z = tf.placeholder('float32', [self.options['batch_size'],\n                              self.options['z_dim']], name='input_noise')\n\n        t_real_classes = tf.placeholder('float32', [self.options['batch_size'],\n                                        self.options['n_classes']],\n                                        name='real_classes')\n\n        t_wrong_classes = tf.placeholder('float32', [self.options['batch_size'],\n                                         self.options['n_classes']],\n                                         name='wrong_classes')\n\n        t_training = tf.placeholder(tf.bool, name='training')\n\n#         BUILDING THE GENERATOR\n        print('Building the Generator')\n        fake_image = self.generator(t_z, t_real_caption,\n                                                 t_training)\n\n#         BUILDING THE DISCRIMINATOR\n        print('Building the Discriminator')\n        disc_real_image, disc_real_image_logits, disc_real_image_aux, \\\n            disc_real_image_aux_logits = self.discriminator(\n                t_real_image, t_real_caption, self.options['n_classes'],\n                t_training)\n\n        disc_wrong_image, disc_wrong_image_logits, disc_wrong_image_aux, \\\n            disc_wrong_image_aux_logits  = self.discriminator(\n                t_wrong_image, t_real_caption, self.options['n_classes'],\n                t_training, reuse = True)\n\n        disc_fake_image, disc_fake_image_logits, disc_fake_image_aux, \\\n            disc_fake_image_aux_logits  = self.discriminator(\n                fake_image, t_real_caption, self.options['n_classes'],\n                t_training, reuse = True)\n\n        d_right_predictions = tf.equal(tf.argmax(disc_real_image_aux, 1),\n                                       tf.argmax(t_real_classes, 1))\n        d_right_accuracy = tf.reduce_mean(tf.cast(d_right_predictions,\n                                                  tf.float32))\n\n        d_wrong_predictions = tf.equal(tf.argmax(disc_wrong_image_aux, 1),\n                                       tf.argmax(t_wrong_classes, 1))\n        d_wrong_accuracy = tf.reduce_mean(tf.cast(d_wrong_predictions,\n                                                  tf.float32))\n\n        d_fake_predictions = tf.equal(tf.argmax(disc_fake_image_aux_logits, 1),\n                                      tf.argmax(t_real_classes, 1))\n        d_fake_accuracy = tf.reduce_mean(tf.cast(d_fake_predictions,\n                                                 tf.float32))\n\n        tf.get_variable_scope()._reuse = False\n\n#         BUILDING THE LOSS FUNCTION FOR THE GENERATOR AND DISCRIMINATOR\n        print('Building the Loss Function')\n        g_loss_1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n                                  logits=disc_fake_image_logits,\n                                  labels=tf.ones_like(disc_fake_image)))\n\n        g_loss_2 = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                                            logits=disc_fake_image_aux_logits,\n                                            labels=t_real_classes))\n\n        d_loss1 = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                                        logits=disc_real_image_logits,\n                                        labels=tf.ones_like(disc_real_image)))\n        d_loss1_1 = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                                            logits=disc_real_image_aux_logits,\n                                            labels=t_real_classes))\n        d_loss2 = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                                    logits=disc_wrong_image_logits,\n                                    labels=tf.zeros_like(disc_wrong_image)))\n        d_loss2_1 = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                                            logits=disc_wrong_image_aux_logits,\n                                            labels=t_wrong_classes))\n        d_loss3 = tf.reduce_mean(\n            tf.nn.sigmoid_cross_entropy_with_logits(\n                                        logits=disc_fake_image_logits,\n                                        labels=tf.zeros_like(disc_fake_image)))\n\n        d_loss = d_loss1 + d_loss1_1 + d_loss2 + d_loss2_1 + d_loss3 + g_loss_2\n\n        g_loss = g_loss_1 + g_loss_2\n\n        t_vars = tf.trainable_variables()\n        \n#         PRINT ALL VALUES: GENERATOR/DISCRIMINATOR LOSS, GENERATOR/DISCRIMINATOR LOSS ON REAL/FAKE IMAGE .ETC\n        print('List of all variables')\n        for v in t_vars:\n            print(v.name)\n            print(v)\n            self.add_histogram_summary(v.name, v)\n\n        self.add_tb_scalar_summaries(d_loss, g_loss, d_loss1, d_loss2, d_loss3,\n              d_loss1_1, d_loss2_1, g_loss_1, g_loss_2, d_right_accuracy,\n              d_wrong_accuracy, d_fake_accuracy)\n\n        self.add_image_summary('Generated Images', fake_image,\n                               self.options['batch_size'])\n\n        d_vars = [var for var in t_vars if 'd_' in var.name]\n        g_vars = [var for var in t_vars if 'g_' in var.name]\n\n        input_tensors = {\n            't_real_image' : t_real_image,\n            't_wrong_image' : t_wrong_image,\n            't_real_caption' : t_real_caption,\n            't_z' : t_z,\n            't_real_classes' : t_real_classes,\n            't_wrong_classes' : t_wrong_classes,\n            't_training' : t_training,\n\n        }\n\n        variables = {\n            'd_vars' : d_vars,\n            'g_vars' : g_vars\n        }\n\n        loss = {\n            'g_loss' : g_loss,\n            'd_loss' : d_loss\n        }\n\n        outputs = {\n            'generator' : fake_image\n        }\n\n        checks = {\n            'd_loss1': d_loss1,\n            'd_loss2': d_loss2,\n            'd_loss3': d_loss3,\n            'g_loss_1': g_loss_1,\n            'g_loss_2': g_loss_2,\n            'd_loss1_1': d_loss1_1,\n            'd_loss2_1': d_loss2_1,\n            'disc_real_image_logits': disc_real_image_logits,\n            'disc_wrong_image_logits': disc_wrong_image,\n            'disc_fake_image_logits': disc_fake_image_logits\n        }\n\n        return input_tensors, variables, loss, outputs, checks\n\n    \n    #ADDING THE METRICS DURING TRAINING FOR EACH EPOCH\n    def add_tb_scalar_summaries(self, d_loss, g_loss, d_loss1, d_loss2,\n                                  d_loss3, d_loss1_1, d_loss2_1, g_loss_1,\n                                  g_loss_2, d_right_accuracy,\n                                  d_wrong_accuracy, d_fake_accuracy):\n\n        self.add_scalar_summary(\"D_Loss\", d_loss)\n        self.add_scalar_summary(\"G_Loss\", g_loss)\n        self.add_scalar_summary(\"D loss-1 [Real/Fake loss for real images]\",\n                                d_loss1)\n        self.add_scalar_summary(\"D loss-2 [Real/Fake loss for wrong images]\",\n                                d_loss2)\n        self.add_scalar_summary(\"D loss-3 [Real/Fake loss for fake images]\",\n                                d_loss3)\n        self.add_scalar_summary(\n            \"D loss-4 [Aux Classifier loss for real images]\", d_loss1_1)\n        self.add_scalar_summary(\n            \"D loss-5 [Aux Classifier loss for wrong images]\", d_loss2_1)\n        self.add_scalar_summary(\"G loss-1 [Real/Fake loss for fake images]\",\n                                g_loss_1)\n        self.add_scalar_summary(\n            \"G loss-2 [Aux Classifier loss for fake images]\", g_loss_2)\n        self.add_scalar_summary(\"Discriminator Real Image Accuracy\",\n                                d_right_accuracy)\n        self.add_scalar_summary(\"Discriminator Wrong Image Accuracy\",\n                                d_wrong_accuracy)\n        self.add_scalar_summary(\"Discriminator Fake Image Accuracy\",\n                                d_fake_accuracy)\n\n#     FUNCTION TO PRINT IN SCALAR FORMAT\n    def add_scalar_summary(self, name, var):\n        with tf.name_scope('summaries'):\n            tf.summary.scalar(name, var)\n\n#     FUNCTION TO PRINT IN HISTGRAM FORMAT\n    def add_histogram_summary(self, name, var):\n        with tf.name_scope('summaries'):\n            tf.summary.histogram(name, var)\n\n#     FUNCTION TO PRINT IN IMAGE FORMAT\n    def add_image_summary(self, name, var, max_outputs=1):\n        with tf.name_scope('summaries'):\n            tf.summary.image(name, var, max_outputs=max_outputs)\n\n    # GENERATOR IMPLEMENTATION FUNCTION\n    def generator(self, t_z, t_text_embedding, t_training):\n\n        s = self.options['image_size']\n        s2, s4, s8, s16 = int(s / 2), int(s / 4), int(s / 8), int(s / 16)\n\n        reduced_text_embedding = lrelu(\n            linear(t_text_embedding, self.options['t_dim'], 'g_embedding'))\n        z_concat = tf.concat([t_z, reduced_text_embedding], -1)\n        z_ = linear(z_concat, self.options['gf_dim'] * 8 * s16 * s16,\n                        'g_h0_lin')\n        h0 = tf.reshape(z_, [-1, s16, s16, self.options['gf_dim'] * 8])\n        h0 = tf.nn.relu(slim.batch_norm(h0, is_training = t_training,\n                                        scope=\"g_bn0\"))\n\n        h1 = deconv2d(h0, [self.options['batch_size'], s8, s8,\n                               self.options['gf_dim'] * 4], name = 'g_h1')\n        h1 = tf.nn.relu(slim.batch_norm(h1, is_training = t_training,\n                                        scope=\"g_bn1\"))\n\n        h2 = deconv2d(h1, [self.options['batch_size'], s4, s4,\n                               self.options['gf_dim'] * 2], name = 'g_h2')\n        h2 = tf.nn.relu(slim.batch_norm(h2, is_training = t_training,\n                                        scope=\"g_bn2\"))\n\n        h3 = deconv2d(h2, [self.options['batch_size'], s2, s2,\n                               self.options['gf_dim'] * 1], name = 'g_h3')\n        h3 = tf.nn.relu(slim.batch_norm(h3, is_training = t_training,\n                                        scope=\"g_bn3\"))\n\n        h4 = deconv2d(h3, [self.options['batch_size'], s, s, 3],\n                          name = 'g_h4')\n        return (tf.tanh(h4) / 2. + 0.5)\n\n\n    # DISCRIMINATOR IMPLEMENTATION FUNCTION\n    \n    def discriminator(self, image, t_text_embedding, n_classes, t_training,\n                      reuse = False) :\n        if reuse :\n            tf.get_variable_scope().reuse_variables()\n\n        h0 = lrelu(\n            conv2d(image, self.options['df_dim'], name = 'd_h0_conv'))  # 64\n\n        h1 = lrelu(slim.batch_norm(conv2d(h0,\n                                             self.options['df_dim'] * 2,\n                                             name = 'd_h1_conv'),\n                                       reuse=reuse,\n                                       is_training = t_training,\n                                       scope = 'd_bn1'))  # 32\n\n        h2 = lrelu(slim.batch_norm(conv2d(h1,\n                                             self.options['df_dim'] * 4,\n                                             name = 'd_h2_conv'),\n                                       reuse=reuse,\n                                       is_training = t_training,\n                                       scope = 'd_bn2'))  # 16\n        h3 = lrelu(slim.batch_norm(conv2d(h2,\n                                             self.options['df_dim'] * 8,\n                                             name = 'd_h3_conv'),\n                                       reuse=reuse,\n                                       is_training = t_training,\n                                       scope = 'd_bn3'))  # 8\n        h3_shape = h3.get_shape().as_list()\n        # ADD TEXT EMBEDDING TO THE NETWORK\n        reduced_text_embeddings = lrelu(linear(t_text_embedding,\n                                                       self.options['t_dim'],\n                                                       'd_embedding'))\n        reduced_text_embeddings = tf.expand_dims(reduced_text_embeddings, 1)\n        reduced_text_embeddings = tf.expand_dims(reduced_text_embeddings, 2)\n        tiled_embeddings = tf.tile(reduced_text_embeddings,\n                                   [1, h3_shape[1], h3_shape[1], 1],\n                                   name = 'tiled_embeddings')\n\n        h3_concat = tf.concat([h3, tiled_embeddings], 3, name = 'h3_concat')\n        h3_new = lrelu(slim.batch_norm(conv2d(h3_concat,\n                                                self.options['df_dim'] * 8,\n                                                      1, 1, 1, 1,\n                                                name = 'd_h3_conv_new'),\n                                        reuse=reuse,\n                                        is_training = t_training,\n                                        scope = 'd_bn4'))  # 4\n\n        h3_flat = tf.reshape(h3_new, [self.options['batch_size'], -1])\n\n        h4 = linear(h3_flat, 1, 'd_h4_lin_rw')\n        h4_aux = linear(h3_flat, n_classes, 'd_h4_lin_ac')\n\n        return tf.nn.sigmoid(h4), h4, tf.nn.sigmoid(h4_aux), h4_aux\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T22:17:54.653791Z","iopub.execute_input":"2023-03-30T22:17:54.654489Z","iopub.status.idle":"2023-03-30T22:17:54.700610Z","shell.execute_reply.started":"2023-03-30T22:17:54.654082Z","shell.execute_reply":"2023-03-30T22:17:54.699394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## FUNCTION TO PERFORM TRAINING OF GAN MODEL","metadata":{}},{"cell_type":"code","source":"# INITIALIZING THE PARAMETERS\n\n#     PARAMEETERS USED\n#     z_dim : Noise dimension\n#     t_dim : Text feature dimension\n#     image_size : Image Dimension\n#     gf_dim : Number of conv in the first layer generator\n#     df_dim : Number of conv in the first layer discriminator\n#     gfc_dim : Dimension of gen untis for for fully connected layer\n#     caption_vector_length : Caption Vector Length\n#     batch_size : Batch Size\n\n\nz_dim=100\n\nt_dim=256\n\nbatch_size=64\n\nimage_size=128\n\ngf_dim=64\n\ndf_dim=64\n\ncaption_vector_length=4800\n\nn_classes = 102 #THAT IS IN FLOWER'S DATASET WE DOWNLOAD THERE WERE 102 CLASSES OF FLOWERS\n\n#LOCATION OF THE DATA\ndata_dir=\"/kaggle/input/generative-text-to-image-synthesis-part-1/Data/Flowers\" \n\n#LOCATION OF THE DATA TO STORE IMAGE GENERATED\ndata_dir_output='Data/Flowers'\n\nlearning_rate=0.0002 #LEARNING RATE OF MODEL\n\nbeta1=0.5\n\nepochs=25 #NUMBER OF EPOCHS\n\nsave_every=30 #SAVE IMAGES GENERATED BY MODEL FROM SENTECES AFETR EVERY 30 BATCHES IN EACH EPOCH \n                     \n\nresume_model=False #WANT TO RESUME MODEL TRAINING AFTER A CERYAIN EPOCH\n\ndata_set= \"flowers\" #DATASET NAME\n\nmodel_name=\"GENERATIVE TEXT TO IMAGE SYNTHESIS GAN MODEL\" #NAME TO GIVE MODEL\n\ntrain = True\n\n# MAIN FUNCTIOON FOR THE MODEL TRAINING\ndef main():\n    \n    \n#     LOADING MODEL PARAMETERS AND DIRECTORIES\n    model_dir, model_chkpnts_dir, model_samples_dir, model_val_samples_dir,\\\n                            model_summaries_dir = initialize_directories()\n\n    datasets_root_dir = join(data_dir, '')\n    loaded_data = load_training_data(datasets_root_dir, data_set,\n                                     caption_vector_length,\n                                     n_classes)\n    model_options = {\n        'z_dim': z_dim,\n        't_dim': t_dim,\n        'batch_size': batch_size,\n        'image_size': image_size,\n        'gf_dim': gf_dim,\n        'df_dim': df_dim,\n        'caption_vector_length': caption_vector_length,\n        'n_classes': loaded_data['n_classes']\n    }\n\n    # Initialize and build the GAN model\n    gan = GAN(model_options)\n    input_tensors, variables, loss, outputs, checks = gan.build_model()\n\n#     MODEL OPTIMIZER'S FOR DISCRIMINATORE\n    d_optim = tf.train.AdamOptimizer(learning_rate,\n                                     beta1=beta1).minimize(loss['d_loss'],\n                                            var_list=variables['d_vars'])\n#     MODEL OPTIMIZER'S FOR GENERATOR\n    g_optim = tf.train.AdamOptimizer(learning_rate,\n                                     beta1=beta1).minimize(loss['g_loss'],\n                                            var_list=variables['g_vars'])\n\n    global_step_tensor = tf.Variable(1, trainable=False, name='global_step')\n    merged = tf.summary.merge_all()\n    sess = tf.InteractiveSession()\n\n#     OBJECT FOR SAVING THE MODEL TRAIN HISTORY AS SUMMARY\n    summary_writer = tf.summary.FileWriter(model_summaries_dir, sess.graph)\n\n    tf.global_variables_initializer().run()\n    saver = tf.train.Saver(max_to_keep=10000)\n\n#     CHECK WHETHER TO START TRAINING FROM BEGINNING OR RESUME FROM LAST EPOCH'S\n    if resume_model:\n        print('Trying to resume training from a previous checkpoint' +\n              str(tf.train.latest_checkpoint(model_chkpnts_dir)))\n        if tf.train.latest_checkpoint(model_chkpnts_dir) is not None:\n            saver.restore(sess, tf.train.latest_checkpoint(model_chkpnts_dir))\n            print('Successfully loaded model. Resuming training.')\n        else:\n            print('Could not load checkpoints.  Training a new model')\n    global_step = global_step_tensor.eval()\n    gs_assign_op = global_step_tensor.assign(global_step)\n    for i in range(epochs):\n        batch_no = 0\n        while batch_no * batch_size + batch_size < \\\n                loaded_data['data_length']:\n\n            real_images, wrong_images, caption_vectors, z_noise, image_files, \\\n            real_classes, wrong_classes, image_caps, image_ids = \\\n                               get_training_batch(batch_no, batch_size,\n                                                  image_size, z_dim,\n                                                  'train', datasets_root_dir,\n                                                  data_set, loaded_data)\n\n            # DISCR UPDATE\n            check_ts = [checks['d_loss1'], checks['d_loss2'],\n                        checks['d_loss3'], checks['d_loss1_1'],\n                        checks['d_loss2_1']]\n\n            feed = {\n                input_tensors['t_real_image'].name : real_images,\n                input_tensors['t_wrong_image'].name : wrong_images,\n                input_tensors['t_real_caption'].name : caption_vectors,\n                input_tensors['t_z'].name : z_noise,\n                input_tensors['t_real_classes'].name : real_classes,\n                input_tensors['t_wrong_classes'].name : wrong_classes,\n                input_tensors['t_training'].name : train\n            }\n\n            _, d_loss, gen, d1, d2, d3, d4, d5= sess.run([d_optim,\n                        loss['d_loss'],outputs['generator']] + check_ts,\n                        feed_dict=feed)\n\n#             printing the status of model training\n            print(\"D total loss: {}\\n\"\n                  \"D loss-1 [Real/Fake loss for real images] : {} \\n\"\n                  \"D loss-2 [Real/Fake loss for wrong images]: {} \\n\"\n                  \"D loss-3 [Real/Fake loss for fake images]: {} \\n\"\n                  \"D loss-4 [Aux Classifier loss for real images]: {} \\n\"\n                  \"D loss-5 [Aux Classifier loss for wrong images]: {}\"\n                  \" \".format(d_loss, d1, d2, d3, d4, d5))\n\n            # UPDATING THE GENRATOR LOSSE'S\n            _, g_loss, gen = sess.run([g_optim, loss['g_loss'],\n                                       outputs['generator']], feed_dict=feed)\n            \n\n            # UPDATING THE GENRATOR LOSSE'S SECOND TIME\n            \n            _, summary, g_loss, gen, g1, g2 = sess.run([g_optim, merged,\n                   loss['g_loss'], outputs['generator'], checks['g_loss_1'],\n                   checks['g_loss_2']], feed_dict=feed)\n            \n            \n#             SAVING THE METRICS OF GENERATOR/DISCRIMINATOR AND THERE LOSSE'S\n            summary_writer.add_summary(summary, global_step)\n            print(\"\\n\\nLOSSES\\nDiscriminator Loss: {}\\nGenerator Loss: {\"\n                  \"}\\nBatch Number: {}\\nEpoch: {},\\nTotal Batches per \"\n                  \"epoch: {}\\n\".format( d_loss, g_loss, batch_no, i,\n                    int(len(loaded_data['image_list']) / batch_size)))\n            print(\"\\nG loss-1 [Real/Fake loss for fake images] : {} \\n\"\n                  \"G loss-2 [Aux Classifier loss for fake images]: {} \\n\"\n                  \" \".format(g1, g2))\n            global_step += 1\n            sess.run(gs_assign_op)\n            batch_no += 1\n#             SAVING THE IMAGES CREATED BY MODEL FROM THE CAPTIONS AND THE CHECKPOINT FOR MODEL TRAINING\n            if (batch_no % save_every) == 0 and batch_no != 0:\n                print(\"Saving Images and the Model\\n\\n\")\n\n                save_for_vis(model_samples_dir, real_images, gen, image_files,\n                             image_caps, image_ids)\n                save_path = saver.save(sess,\n                                       join(model_chkpnts_dir,\n                                            \"latest_model_{}_temp.ckpt\".format(\n                                                data_set)))\n\n                # Getting a batch for validation of the model ggenerated iamegs\n                val_captions, val_image_files, val_image_caps, val_image_ids = \\\n                          get_val_caps_batch(batch_size, loaded_data,\n                                             data_set, datasets_root_dir)\n\n                shutil.rmtree(model_val_samples_dir)\n                os.makedirs(model_val_samples_dir)\n\n                for val_viz_cnt in range(0, 4):\n                    val_z_noise = np.random.uniform(-1, 1, [batch_size,\n                                                            z_dim])\n\n                    val_feed = {\n                        input_tensors['t_real_caption'].name : val_captions,\n                        input_tensors['t_z'].name : val_z_noise,\n                        input_tensors['t_training'].name : True\n                    }\n\n                    val_gen = sess.run([outputs['generator']],\n                                       feed_dict=val_feed)\n                    save_for_viz_val(model_val_samples_dir, val_gen,\n                                     val_image_files, val_image_caps,\n                                     val_image_ids, image_size,\n                                     val_viz_cnt)\n\n        # Save the model after every epoch\n        if i % 1 == 0:\n            epoch_dir = join(model_chkpnts_dir, str(i))\n            if not os.path.exists(epoch_dir):\n                os.makedirs(epoch_dir)\n\n            save_path = saver.save(sess,\n                                   join(epoch_dir,\n                                        \"model_after_{}_epoch_{}.ckpt\".\n                                            format(data_set, i)))\n            val_captions, val_image_files, val_image_caps, val_image_ids = \\\n                  get_val_caps_batch(batch_size, loaded_data,\n                                     data_set, datasets_root_dir)\n\n            shutil.rmtree(model_val_samples_dir)\n            os.makedirs(model_val_samples_dir)\n\n            for val_viz_cnt in range(0, 10):\n                val_z_noise = np.random.uniform(-1, 1, [batch_size,\n                                                        z_dim])\n                val_feed = {\n                    input_tensors['t_real_caption'].name : val_captions,\n                    input_tensors['t_z'].name : val_z_noise,\n                    input_tensors['t_training'].name : True\n                }\n                val_gen = sess.run([outputs['generator']], feed_dict=val_feed)\n                save_for_viz_val(model_val_samples_dir, val_gen,\n                                 val_image_files, val_image_caps,\n                                 val_image_ids, image_size,\n                                 val_viz_cnt)\n\n# FUNCTION TO LOAD TRAIN DATA\ndef load_training_data(data_dir, data_set, caption_vector_length, n_classes) :\n    if data_set == 'flowers' :\n        flower_str_captions = pickle.load(\n            open(join(data_dir, 'pickles/flowers_caps.pkl'), \"rb\"))\n\n        img_classes = pickle.load(\n            open(join(data_dir, 'pickles/flower_tc.pkl'), \"rb\"))\n\n        flower_enc_captions = pickle.load(\n            open(join(data_dir, 'pickles/flower_tv.pkl'), \"rb\"))\n        tr_image_ids = pickle.load(\n            open(join(data_dir, 'pickles/train_ids.pkl'), \"rb\"))\n        val_image_ids = pickle.load(\n            open(join(data_dir, 'pickles/val_ids.pkl'), \"rb\"))\n\n        max_caps_len = caption_vector_length\n        tr_n_imgs = len(tr_image_ids)\n        val_n_imgs = len(val_image_ids)\n\n        return {\n            'image_list'    : tr_image_ids,\n            'captions'      : flower_enc_captions,\n            'data_length'   : tr_n_imgs,\n            'classes'       : img_classes,\n            'n_classes'     : n_classes,\n            'max_caps_len'  : max_caps_len,\n            'val_img_list'  : val_image_ids,\n            'val_captions'  : flower_enc_captions,\n            'val_data_len'  : val_n_imgs,\n            'str_captions'  : flower_str_captions\n        }\n\n    else :\n        raise Exception('No Dataset Found')\n\n# FUNCTION TO CRAETE THE DIRECTORIES FOR STORING THE CHECKPOINT'S, GENERATED IMAGES, SUMMARIES\ndef initialize_directories():\n    model_dir = join(data_dir_output, 'training', model_name)\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_chkpnts_dir = join(model_dir, 'checkpoints')\n    if not os.path.exists(model_chkpnts_dir):\n        os.makedirs(model_chkpnts_dir)\n\n    model_summaries_dir = join(model_dir, 'summaries')\n    if not os.path.exists(model_summaries_dir):\n        os.makedirs(model_summaries_dir)\n\n    model_samples_dir = join(model_dir, 'samples')\n    if not os.path.exists(model_samples_dir):\n        os.makedirs(model_samples_dir)\n\n    model_val_samples_dir = join(model_dir, 'val_samples')\n    if not os.path.exists(model_val_samples_dir):\n        os.makedirs(model_val_samples_dir)\n\n    return model_dir, model_chkpnts_dir, model_samples_dir, \\\n           model_val_samples_dir, model_summaries_dir\n\n# FUNCTIONS FOR SAVING THE IMAGES GENERATED BY THE IMAGE CAPTIONS AND THE GENERATOR ALSO\ndef save_for_viz_val(data_dir, generated_images, image_files, image_caps,\n                     image_ids, image_size, id):\n\n    generated_images = np.squeeze(np.array(generated_images))\n    for i in range(0, generated_images.shape[0]) :\n        image_dir = join(data_dir, str(image_ids[i]))\n        if not os.path.exists(image_dir):\n            os.makedirs(image_dir)\n\n        real_image_path = join(image_dir,\n                               '{}.jpg'.format(image_ids[i]))\n        if os.path.exists(image_dir):\n            real_images_255 = load_image_array(image_files[i],\n                                        image_size, image_ids[i], mode='val')\n            imageio.imwrite(real_image_path, real_images_255)\n\n        caps_dir = join(image_dir, \"caps.txt\")\n        if not os.path.exists(caps_dir):\n            with open(caps_dir, \"w\") as text_file:\n                text_file.write(image_caps[i]+\"\\n\")\n\n        fake_images_255 = generated_images[i]\n        imageio.imwrite(join(image_dir, 'fake_image_{}.jpg'.format(id)),\n                          fake_images_255)\n\n# FUNCTIONS FOR SAVING THE IMAGES GENERATED BY THE IMAGE CAPTIONS AND THE GENERATOR ALSO\ndef save_for_vis(data_dir, real_images, generated_images, image_files,\n                 image_caps, image_ids) :\n\n    shutil.rmtree(data_dir)\n    os.makedirs(data_dir)\n\n    for i in range(0, real_images.shape[0]) :\n        real_images_255 = (real_images[i, :, :, :])\n        imageio.imwrite(join(data_dir,\n               '{}_{}.jpg'.format(i, image_files[i].split('/')[-1])),\n                          real_images_255)\n\n        fake_images_255 = (generated_images[i, :, :, :])\n        imageio.imwrite(join(data_dir, 'fake_image_{}.jpg'.format(\n            i)), fake_images_255)\n\n    str_caps = '\\n'.join(image_caps)\n    str_image_ids = '\\n'.join([str(image_id) for image_id in image_ids])\n    with open(join(data_dir, \"caps.txt\"), \"w\") as text_file:\n        text_file.write(str_caps)\n    with open(join(data_dir, \"ids.txt\"), \"w\") as text_file:\n        text_file.write(str_image_ids)\n\n# FUNCTION TO GET CAPTION'S BATCH FOR VALIDATION\ndef get_val_caps_batch(batch_size, loaded_data, data_set, data_dir):\n    if data_set == 'flowers':\n        captions = np.zeros((batch_size, loaded_data['max_caps_len']))\n\n        batch_idx = np.random.randint(0, loaded_data['val_data_len'],\n                                      size = batch_size)\n        image_ids = np.take(loaded_data['val_img_list'], batch_idx)\n        image_files = []\n        image_caps = []\n        for idx, image_id in enumerate(image_ids) :\n            image_file = join(data_dir,\n                              '102flowers/jpg/' + image_id)\n            random_caption = random.randint(0, 4)\n            captions[idx, :] = \\\n                loaded_data['val_captions'][image_id][random_caption][\n                0 :loaded_data['max_caps_len']]\n\n            image_caps.append(loaded_data['str_captions']\n                              [image_id][random_caption])\n            image_files.append(image_file)\n\n        return captions, image_files, image_caps, image_ids\n    else:\n        raise Exception('Dataset not found')\n\n# FUNCTION TO GET CAPTION'S BATCH FOR TRAINING\ndef get_training_batch(batch_no, batch_size, image_size, z_dim, split,\n                       data_dir, data_set, loaded_data = None) :\n    \n    if data_set == 'flowers':\n        real_images = np.zeros((batch_size, image_size, image_size, 3))\n        wrong_images = np.zeros((batch_size, image_size, image_size, 3))\n        captions = np.zeros((batch_size, loaded_data['max_caps_len']))\n        real_classes = np.zeros((batch_size, loaded_data['n_classes']))\n        wrong_classes = np.zeros((batch_size, loaded_data['n_classes']))\n\n        cnt = 0\n        image_files = []\n        image_caps = []\n        image_ids = []\n        for i in range(batch_no * batch_size,\n                       batch_no * batch_size + batch_size) :\n            idx = i % len(loaded_data['image_list'])\n            image_file = join(data_dir,\n                              '102flowers/jpg/' + loaded_data['image_list'][idx])\n\n            image_ids.append(loaded_data['image_list'][idx])\n\n            image_array = load_image_array_flowers(image_file,\n                                                            image_size)\n            real_images[cnt, :, :, :] = image_array\n\n            # Improve this selection of wrong image\n            wrong_image_id = random.randint(0,\n                                            len(loaded_data['image_list']) - 1)\n            wrong_image_file = join(data_dir,\n                                    '102flowers/jpg/' + loaded_data['image_list'][\n                                                            wrong_image_id])\n            wrong_image_array = load_image_array_flowers(wrong_image_file,\n                                                                  image_size)\n            wrong_images[cnt, :, :, :] = wrong_image_array\n\n            wrong_classes[cnt, :] = loaded_data['classes'][loaded_data['image_list'][\n                                    wrong_image_id]][0 :loaded_data['n_classes']]\n\n            random_caption = random.randint(0, 4)\n            captions[cnt, :] = \\\n            loaded_data['captions'][loaded_data['image_list'][idx]][\n                                random_caption][0 :loaded_data['max_caps_len']]\n\n            real_classes[cnt, :] = \\\n                loaded_data['classes'][loaded_data['image_list'][idx]][\n                                                0 :loaded_data['n_classes']]\n            str_cap = loaded_data['str_captions'][loaded_data['image_list']\n                                [idx]][random_caption]\n\n            image_files.append(image_file)\n            image_caps.append(str_cap)\n            cnt += 1\n\n        z_noise = np.random.uniform(-1, 1, [batch_size, z_dim])\n        return real_images, wrong_images, captions, z_noise, image_files, \\\n               real_classes, wrong_classes, image_caps, image_ids\n    else:\n        raise Exception('Dataset not found')\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-30T22:17:54.702534Z","iopub.execute_input":"2023-03-30T22:17:54.702930Z","iopub.status.idle":"2023-03-30T22:17:54.764764Z","shell.execute_reply.started":"2023-03-30T22:17:54.702890Z","shell.execute_reply":"2023-03-30T22:17:54.763536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## STARTING THE TRAINING OF THE GAN MODEL","metadata":{}},{"cell_type":"code","source":"# FUNCTION CALLING THE MAIN TRAINING FUNCTION\nmain()","metadata":{"execution":{"iopub.status.busy":"2023-03-30T22:18:06.095102Z","iopub.execute_input":"2023-03-30T22:18:06.095908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SHOWING THE RESULTS IN TENSORBOARD","metadata":{}},{"cell_type":"code","source":"#UPLOADING THE SUMMARY OF THE TRAINING TILL 25 EPOCH'S TO TENSORBOARD\n\n!timeout 30m bash -c \"echo yes | tensorboard dev upload\\\n--logdir 'Data/Flowers/training/GENERATIVE TEXT TO IMAGE SYNTHESIS GAN MODEL/summaries'\\\n--name 'GENERATIVE TEXT TO IMAGE SYNTHESIS PART 2 EPOCH 25 RESULTS.'\\\n--description 'THIS IS THE SUMMARY FOR THE RESULTS UPTO 25th EPOCH.'\"","metadata":{"execution":{"iopub.status.busy":"2023-03-31T14:14:57.003855Z","iopub.execute_input":"2023-03-31T14:14:57.004282Z","iopub.status.idle":"2023-03-31T14:15:06.640877Z","shell.execute_reply.started":"2023-03-31T14:14:57.004239Z","shell.execute_reply":"2023-03-31T14:15:06.639191Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\n***** TensorBoard Uploader *****\n\nThis will upload your TensorBoard logs to https://tensorboard.dev/ from\nthe following directory:\n\nData/Flowers/training/GENERATIVE TEXT TO IMAGE SYNTHESIS GAN MODEL/summaries\n\nThis TensorBoard will be visible to everyone. Do not upload sensitive\ndata.\n\nYour use of this service is subject to Google's Terms of Service\n<https://policies.google.com/terms> and Privacy Policy\n<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n<https://tensorboard.dev/policy/terms/>.\n\nThis notice will not be shown again while you are logged into the uploader.\nTo log out, run `tensorboard dev auth revoke`.\n\nContinue? (yes/NO) \nTo sign in with the TensorBoard uploader:\n\n1. On your computer or phone, visit:\n\n   https://www.google.com/device\n\n2. Sign in with your Google account, then enter:\n\n   YBJM-BVPJ\n\n^C\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 308, in run\n    _run_main(main, args)\n  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 254, in _run_main\n    sys.exit(main(argv))\n  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/program.py\", line 276, in main\n    return runner(self.flags) or 0\n  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/uploader/uploader_subcommand.py\", line 691, in run\n    return _run(flags, self._experiment_url_callback)\n  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/uploader/uploader_subcommand.py\", line 95, in _run\n    force_console=flags.auth_force_console\n  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/uploader/auth.py\", line 243, in authenticate_user\n    return flow.run()\n  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/uploader/auth.py\", line 280, in run\n    expiration_seconds=device_response[\"expires_in\"],\n  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/uploader/auth.py\", line 319, in _poll_for_auth_token\n    time.sleep(polling_interval)\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/bin/tensorboard\", line 10, in <module>\n    sys.exit(run_main())\n  File \"/opt/conda/lib/python3.7/site-packages/tensorboard/main.py\", line 46, in run_main\n    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n  File \"/opt/conda/lib/python3.7/site-packages/absl/app.py\", line 312, in run\n    exc = sys.exc_info()[1]\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# ------------------------------------------------THE END------------------------------------------------","metadata":{}}]}